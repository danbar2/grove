# Workload: SP8 - Large Scaling Ratio
# Test scenario: PCS with block constraint, PCSG with replicas=10/minAvailable=3, PCLQ with host constraint
---
apiVersion: grove.io/v1alpha1
kind: PodCliqueSet
metadata:
  name: tas-large-scale
  labels:
    app: tas-large-scale
spec:
  replicas: 1
  template:
    topologyConstraint:
      packDomain: block
    podCliqueScalingGroups:
      - name: workers
        replicas: 10
        minAvailable: 3
        topologyConstraint:
        cliqueNames:
          - worker
    cliques:
      - name: worker
        labels:
          kai.scheduler/queue: test
        topologyConstraint:
          packDomain: host
        spec:
          roleName: worker
          replicas: 2
          minAvailable: 2
          podSpec:
            schedulerName: kai-scheduler
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                    - matchExpressions:
                        - key: node_role.e2e.grove.nvidia.com
                          operator: In
                          values:
                            - agent
            tolerations:
              - key: node_role.e2e.grove.nvidia.com
                operator: Equal
                value: agent
                effect: NoSchedule
            containers:
              - name: worker
                image: registry:5001/busybox:latest
                command: ["sleep", "infinity"]
                resources:
                  requests:
                    memory: 40Mi
